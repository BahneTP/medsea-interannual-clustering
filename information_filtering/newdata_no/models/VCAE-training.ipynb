{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4951ac0e-4f7b-4c6d-87e8-fa212d1750a9",
   "metadata": {
    "id": "4951ac0e-4f7b-4c6d-87e8-fa212d1750a9"
   },
   "source": [
    "# Variational Convolutional Autoencoder\n",
    "\n",
    "This notebook uses a **Convolutional Variational Autoencoder (CVAE)**.  \n",
    "It combines the strengths of a **Convolutional Autoencoder**, which captures local spatial patterns in maps, with the **Variational Autoencoder** approach that maps each input to a smooth probability distribution in latent space.  \n",
    "This way, we compress oceanographic maps into robust, compact representations that stay comparable and regularized.  \n",
    "Masking handles missing or irrelevant regions, and the structured latent space makes it easier to cluster or analyze similar patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b2f90b1-6826-4a2b-896b-f0f59aa9999c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 110274,
     "status": "ok",
     "timestamp": 1750955287591,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "4b2f90b1-6826-4a2b-896b-f0f59aa9999c",
    "outputId": "4dec4adf-f65f-4eea-9962-5fa5887df0ec"
   },
   "outputs": [],
   "source": [
    "!pip install -q -r ../../requirements.txt &> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a9a3b7-de75-47f5-8e47-612a35397ee0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e3c815e-a7d9-46d6-a45d-c64b8ed20a69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1750955461797,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "7e3c815e-a7d9-46d6-a45d-c64b8ed20a69",
    "outputId": "c8c3bc6e-81b3-473b-e58c-d53fa46ceb8b"
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, Dataset\n",
    "from torchinfo import summary\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import helper_functions\n",
    "import importlib\n",
    "importlib.reload(helper_functions)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "456d58da-0454-4ae0-b025-ea0d3a7f8efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 27\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "trend_removal=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f10a58f-d84f-4be4-abeb-478f20b1b76f",
   "metadata": {},
   "source": [
    "Since our ConvNet works with masked map data, it naturally produces some NaNs — that's expected behavior. To avoid flooding the output with warnings, we've disabled them here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec2175a-a4a6-4443-89a4-c30147743c2e",
   "metadata": {
    "id": "cec2175a-a4a6-4443-89a4-c30147743c2e"
   },
   "source": [
    "## Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "974f3cdc-1c88-4f71-88ee-bb7d3fe13362",
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1750955397581,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "974f3cdc-1c88-4f71-88ee-bb7d3fe13362"
   },
   "outputs": [],
   "source": [
    "class MaskedDataset(Dataset):\n",
    "    def __init__(self, X, M):\n",
    "        self.X = X\n",
    "        self.M = M\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.M[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50969759-8f23-4985-acc6-3ada84575eae",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39c51e16-de8e-4908-a971-220fe123e743",
   "metadata": {
    "executionInfo": {
     "elapsed": 874,
     "status": "ok",
     "timestamp": 1750955398447,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "36cff7de-fbe6-4edb-9c94-5fead6c7a13a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 461\n"
     ]
    }
   ],
   "source": [
    "ds_train = xr.open_dataset(\"../../data/medsea1987to2025_train.nc\")\n",
    "\n",
    "X_np, M_np = helper_functions.preprocessing_conv(ds_train, [\"thetao\", \"so\"], [50, 300, 1000], trend_removal, 1)\n",
    "X_train = torch.tensor(X_np)\n",
    "M_train = torch.tensor(M_np)\n",
    "train_dataset = MaskedDataset(X_train, M_train)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    generator=torch.Generator().manual_seed(SEED)\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb22c94-4a73-4f80-85e7-dbb481adb48a",
   "metadata": {},
   "source": [
    "### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "689fa3ae-4fab-467b-8d25-484607097e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set size: 184\n"
     ]
    }
   ],
   "source": [
    "ds_val = xr.open_dataset(\"../../data/medsea1987to2025_val.nc\")\n",
    "\n",
    "X_np, M_np = helper_functions.preprocessing_conv(ds_train, [\"thetao\", \"so\"], [50, 300, 1000], trend_removal, 1)\n",
    "X_val = torch.tensor(X_np)\n",
    "M_val = torch.tensor(M_np)\n",
    "val_dataset = MaskedDataset(X_val, M_val)\n",
    "\n",
    "val_size = int(0.4 * len(val_dataset))\n",
    "_ , val_subset = torch.utils.data.random_split(\n",
    "    val_dataset,\n",
    "    [len(val_dataset) - val_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(SEED)\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_subset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Validation set size: {len(val_subset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed51ed5-9af0-42ad-a1a9-9d9b0b5edfe7",
   "metadata": {
    "id": "4ed51ed5-9af0-42ad-a1a9-9d9b0b5edfe7"
   },
   "source": [
    "## The Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4956a2-917d-4d60-afc2-410b41767582",
   "metadata": {},
   "source": [
    "Again, we include all depths and features for reconstruction. The difference in our ConvNet is that these features are not simply concatenated, but stacked as channels — similar to how RGB channels work in images. This allows the network to capture spatial relationships between features and depths more effectively, which might be important in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "BmPchZzRmXID",
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1750955534808,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "BmPchZzRmXID"
   },
   "outputs": [],
   "source": [
    "class VCAE(nn.Module):\n",
    "    def __init__(self, in_channels, latent_dim=3, dropout_p=0.2, channels=[32, 64, 128, 256, 512, 1024], input_shape=(190, 508)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.channels = channels\n",
    "\n",
    "        encoder_layers = []\n",
    "        prev_channels = in_channels\n",
    "        h, w = input_shape\n",
    "\n",
    "        for ch in channels:\n",
    "            encoder_layers += [\n",
    "                nn.Conv2d(prev_channels, ch, kernel_size=3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(ch),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout2d(p=dropout_p),\n",
    "            ]\n",
    "            prev_channels = ch\n",
    "            h = math.floor((h + 2 * 1 - 3) / 2 + 1)\n",
    "            w = math.floor((w + 2 * 1 - 3) / 2 + 1)\n",
    "\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        self.unflatten_shape = (channels[-1], h, w)\n",
    "\n",
    "        flat_dim = channels[-1] * h * w\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_shared = nn.Sequential(\n",
    "            nn.Linear(flat_dim, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(1024, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "        )\n",
    "        self.fc_mean = nn.Linear(64, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(64, latent_dim)\n",
    "\n",
    "        self.fc_dec = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(64, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(1024, flat_dim)\n",
    "        )\n",
    "\n",
    "        decoder_layers = []\n",
    "        rev_channels = list(reversed(channels))\n",
    "        for i in range(len(rev_channels) - 1):\n",
    "            decoder_layers += [\n",
    "                nn.ConvTranspose2d(rev_channels[i], rev_channels[i + 1], kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(rev_channels[i + 1]),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout2d(p=dropout_p),\n",
    "            ]\n",
    "\n",
    "        decoder_layers += [\n",
    "            nn.ConvTranspose2d(rev_channels[-1], in_channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        ]\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        if mask is not None:\n",
    "            x = x * mask\n",
    "        x = self.encoder(x)\n",
    "        h = self.fc_shared(self.flatten(x))\n",
    "        z_mean = self.fc_mean(h)\n",
    "        z_logvar = self.fc_logvar(h)\n",
    "        z = self.reparameterize(z_mean, z_logvar)\n",
    "        x = self.fc_dec(z)\n",
    "        x = x.view(x.size(0), *self.unflatten_shape)\n",
    "        x = self.decoder(x)\n",
    "        return x[:, :, :self.input_shape[0], :self.input_shape[1]], z_mean, z_logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd997601-9a5d-4a69-a879-fe1c9936e87b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Convolutional Autoencoder\n",
    "\n",
    "We use a simple convolutional autoencoder that compresses the input maps into a small latent vector and reconstructs them back. The encoder reduces spatial resolution through convolutional layers, while the decoder upsamples the data back using transposed convolutions.\n",
    "\n",
    "I experimented with different numbers of layers, kernel sizes, and dropout values. The current setup gave the best trade-off between reconstruction quality and training stability.\n",
    "\n",
    "\n",
    "### Hard Masking\n",
    "- Since many values in the input maps are invalid (e.g. land areas), we apply a **hard mask** before feeding the data into the encoder.\n",
    "- This mask zeroes out irrelevant values, so the model only learns from valid oceanic regions.\n",
    "- During training, the same mask is applied to the loss function to ensure the model is not penalized for errors in masked-out areas.\n",
    "- This is essential for learning robust spatial patterns without being misled by missing or irrelevant data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f29428-7c81-4a80-a415-c051c92d86cd",
   "metadata": {
    "id": "f9f29428-7c81-4a80-a415-c051c92d86cd"
   },
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f36d7cfa-8d22-41f0-a1b0-fb2c31ac8f5e",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1750955398551,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "f36d7cfa-8d22-41f0-a1b0-fb2c31ac8f5e"
   },
   "outputs": [],
   "source": [
    "def masked_recon_loss(x_recon, x_true, mask):\n",
    "    loss = ((x_recon - x_true) ** 2) * mask\n",
    "    return loss.sum() / mask.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696eba88-be08-4790-a145-0a3ddeeffaa2",
   "metadata": {
    "id": "696eba88-be08-4790-a145-0a3ddeeffaa2"
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8b3e058-34ba-439d-aeda-aa0a21e6dd5e",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1750955398556,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "c8b3e058-34ba-439d-aeda-aa0a21e6dd5e"
   },
   "outputs": [],
   "source": [
    "def train(num_epochs: int, kl_annealing_epochs: int = 50, bint: float = 0.1):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_recons = []\n",
    "    val_recons = []\n",
    "    train_kls = []\n",
    "    val_kls = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        beta = min(bint, epoch / kl_annealing_epochs * bint)\n",
    "        model.train()\n",
    "        running_train_recon = 0.0\n",
    "        running_train_kl = 0.0\n",
    "\n",
    "        for x, mask in train_loader:\n",
    "            x = x.to(device)\n",
    "            mask = mask.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            x_recon, z_mean, z_logvar = model(x, mask=mask)\n",
    "            kl_loss = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp(), dim=1).mean()\n",
    "            recon_loss = masked_recon_loss(x_recon, x, mask)\n",
    "            loss = recon_loss + beta * kl_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_recon += recon_loss.item() * x.size(0)\n",
    "            running_train_kl += kl_loss.item() * x.size(0)\n",
    "\n",
    "        train_recon = running_train_recon / len(train_loader.dataset)\n",
    "        train_kl = running_train_kl / len(train_loader.dataset)\n",
    "        train_loss = train_recon + beta * train_kl\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_recons.append(train_recon)\n",
    "        train_kls.append(train_kl)\n",
    "\n",
    "        model.eval()\n",
    "        running_val_recon = 0.0\n",
    "        running_val_kl = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, mask in val_loader:\n",
    "                x = x.to(device)\n",
    "                mask = mask.to(device)\n",
    "                x_recon, z_mean, z_logvar = model(x, mask=mask)\n",
    "                kl_loss = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp(), dim=1).mean()\n",
    "                recon_loss = masked_recon_loss(x_recon, x, mask)\n",
    "                running_val_recon += recon_loss.item() * x.size(0)\n",
    "                running_val_kl += kl_loss.item() * x.size(0)\n",
    "\n",
    "        val_recon = running_val_recon / len(val_loader.dataset)\n",
    "        val_kl = running_val_kl / len(val_loader.dataset)\n",
    "        val_loss = val_recon + beta * val_kl\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        val_recons.append(val_recon)\n",
    "        val_kls.append(val_kl)\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "                f\"β: {beta:.5f} | \"\n",
    "                f\"Train Loss: {train_loss:.4f} (Recon: {train_recon:.4f}, KL: {train_kl:.4f}) | \"\n",
    "                f\"Val Loss: {val_loss:.4f} (Recon: {val_recon:.4f}, KL: {val_kl:.4f})\"\n",
    "            )\n",
    "\n",
    "    return train_losses, val_losses, train_recons, val_recons, train_kls, val_kls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd29feb4-1f80-400e-b1f7-a41f47fcace9",
   "metadata": {
    "id": "bd29feb4-1f80-400e-b1f7-a41f47fcace9"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22fecc03-c5a1-49d2-87d0-9b490e8ff0c2",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1750955398545,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "22fecc03-c5a1-49d2-87d0-9b490e8ff0c2"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36mleCQi1wa1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "executionInfo": {
     "elapsed": 224,
     "status": "error",
     "timestamp": 1750955678818,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "36mleCQi1wa1",
    "outputId": "618c29cd-1083-48b1-b84a-7a9790043df4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VCAE                                     [1, 6, 190, 508]          --\n",
       "├─Sequential: 1-1                        [1, 1024, 3, 8]           --\n",
       "│    └─Conv2d: 2-1                       [1, 32, 95, 254]          1,760\n",
       "│    └─InstanceNorm2d: 2-2               [1, 32, 95, 254]          --\n",
       "│    └─LeakyReLU: 2-3                    [1, 32, 95, 254]          --\n",
       "│    └─Dropout2d: 2-4                    [1, 32, 95, 254]          --\n",
       "│    └─Conv2d: 2-5                       [1, 64, 48, 127]          18,496\n",
       "│    └─InstanceNorm2d: 2-6               [1, 64, 48, 127]          --\n",
       "│    └─LeakyReLU: 2-7                    [1, 64, 48, 127]          --\n",
       "│    └─Dropout2d: 2-8                    [1, 64, 48, 127]          --\n",
       "│    └─Conv2d: 2-9                       [1, 128, 24, 64]          73,856\n",
       "│    └─InstanceNorm2d: 2-10              [1, 128, 24, 64]          --\n",
       "│    └─LeakyReLU: 2-11                   [1, 128, 24, 64]          --\n",
       "│    └─Dropout2d: 2-12                   [1, 128, 24, 64]          --\n",
       "│    └─Conv2d: 2-13                      [1, 256, 12, 32]          295,168\n",
       "│    └─InstanceNorm2d: 2-14              [1, 256, 12, 32]          --\n",
       "│    └─LeakyReLU: 2-15                   [1, 256, 12, 32]          --\n",
       "│    └─Dropout2d: 2-16                   [1, 256, 12, 32]          --\n",
       "│    └─Conv2d: 2-17                      [1, 512, 6, 16]           1,180,160\n",
       "│    └─InstanceNorm2d: 2-18              [1, 512, 6, 16]           --\n",
       "│    └─LeakyReLU: 2-19                   [1, 512, 6, 16]           --\n",
       "│    └─Dropout2d: 2-20                   [1, 512, 6, 16]           --\n",
       "│    └─Conv2d: 2-21                      [1, 1024, 3, 8]           4,719,616\n",
       "│    └─InstanceNorm2d: 2-22              [1, 1024, 3, 8]           --\n",
       "│    └─LeakyReLU: 2-23                   [1, 1024, 3, 8]           --\n",
       "│    └─Dropout2d: 2-24                   [1, 1024, 3, 8]           --\n",
       "├─Flatten: 1-2                           [1, 24576]                --\n",
       "├─Sequential: 1-3                        [1, 64]                   --\n",
       "│    └─Linear: 2-25                      [1, 1024]                 25,166,848\n",
       "│    └─LeakyReLU: 2-26                   [1, 1024]                 --\n",
       "│    └─Dropout: 2-27                     [1, 1024]                 --\n",
       "│    └─Linear: 2-28                      [1, 64]                   65,600\n",
       "│    └─LeakyReLU: 2-29                   [1, 64]                   --\n",
       "│    └─Dropout: 2-30                     [1, 64]                   --\n",
       "├─Linear: 1-4                            [1, 3]                    195\n",
       "├─Linear: 1-5                            [1, 3]                    195\n",
       "├─Sequential: 1-6                        [1, 24576]                --\n",
       "│    └─Linear: 2-31                      [1, 64]                   256\n",
       "│    └─LeakyReLU: 2-32                   [1, 64]                   --\n",
       "│    └─Dropout: 2-33                     [1, 64]                   --\n",
       "│    └─Linear: 2-34                      [1, 1024]                 66,560\n",
       "│    └─LeakyReLU: 2-35                   [1, 1024]                 --\n",
       "│    └─Dropout: 2-36                     [1, 1024]                 --\n",
       "│    └─Linear: 2-37                      [1, 24576]                25,190,400\n",
       "├─Sequential: 1-7                        [1, 6, 192, 512]          --\n",
       "│    └─ConvTranspose2d: 2-38             [1, 512, 6, 16]           4,719,104\n",
       "│    └─InstanceNorm2d: 2-39              [1, 512, 6, 16]           --\n",
       "│    └─LeakyReLU: 2-40                   [1, 512, 6, 16]           --\n",
       "│    └─Dropout2d: 2-41                   [1, 512, 6, 16]           --\n",
       "│    └─ConvTranspose2d: 2-42             [1, 256, 12, 32]          1,179,904\n",
       "│    └─InstanceNorm2d: 2-43              [1, 256, 12, 32]          --\n",
       "│    └─LeakyReLU: 2-44                   [1, 256, 12, 32]          --\n",
       "│    └─Dropout2d: 2-45                   [1, 256, 12, 32]          --\n",
       "│    └─ConvTranspose2d: 2-46             [1, 128, 24, 64]          295,040\n",
       "│    └─InstanceNorm2d: 2-47              [1, 128, 24, 64]          --\n",
       "│    └─LeakyReLU: 2-48                   [1, 128, 24, 64]          --\n",
       "│    └─Dropout2d: 2-49                   [1, 128, 24, 64]          --\n",
       "│    └─ConvTranspose2d: 2-50             [1, 64, 48, 128]          73,792\n",
       "│    └─InstanceNorm2d: 2-51              [1, 64, 48, 128]          --\n",
       "│    └─LeakyReLU: 2-52                   [1, 64, 48, 128]          --\n",
       "│    └─Dropout2d: 2-53                   [1, 64, 48, 128]          --\n",
       "│    └─ConvTranspose2d: 2-54             [1, 32, 96, 256]          18,464\n",
       "│    └─InstanceNorm2d: 2-55              [1, 32, 96, 256]          --\n",
       "│    └─LeakyReLU: 2-56                   [1, 32, 96, 256]          --\n",
       "│    └─Dropout2d: 2-57                   [1, 32, 96, 256]          --\n",
       "│    └─ConvTranspose2d: 2-58             [1, 6, 192, 512]          1,734\n",
       "==========================================================================================\n",
       "Total params: 63,067,148\n",
       "Trainable params: 63,067,148\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 3.10\n",
       "==========================================================================================\n",
       "Input size (MB): 2.32\n",
       "Forward/backward pass size (MB): 29.37\n",
       "Params size (MB): 252.27\n",
       "Estimated Total Size (MB): 283.96\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# torch.cuda.reset_peak_memory_stats()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = VCAE(in_channels=X_np.shape[1], latent_dim=3, dropout_p=0.2, channels= [32, 64, 128, 256, 512, 1024]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0004)\n",
    "\n",
    "summary(model, input_size=(1, X_np.shape[1], 190, 508))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "PUQ6KPZs2CZL",
   "metadata": {
    "id": "PUQ6KPZs2CZL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000 | β: 0.00060 | Train Loss: 0.7305 (Recon: 0.7226, KL: 13.1507) | Val Loss: 0.6687 (Recon: 0.6627, KL: 10.1468)\n",
      "Epoch 20/1000 | β: 0.00127 | Train Loss: 0.6544 (Recon: 0.6414, KL: 10.2626) | Val Loss: 0.6066 (Recon: 0.5937, KL: 10.1607)\n",
      "Epoch 30/1000 | β: 0.00193 | Train Loss: 0.6341 (Recon: 0.6187, KL: 7.9901) | Val Loss: 0.5857 (Recon: 0.5742, KL: 5.9149)\n",
      "Epoch 40/1000 | β: 0.00260 | Train Loss: 0.5997 (Recon: 0.5821, KL: 6.7816) | Val Loss: 0.5414 (Recon: 0.5242, KL: 6.5832)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_losses, val_losses, train_recons, val_recons, train_kls, val_kls \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.04\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_epochs, kl_annealing_epochs, bint)\u001b[0m\n\u001b[1;32m     14\u001b[0m running_train_recon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     15\u001b[0m running_train_kl \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:211\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, train_recons, val_recons, train_kls, val_kls = train(1000, 600, 0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68549efe-8ccf-4412-a9e6-0868f92d4e68",
   "metadata": {
    "id": "68549efe-8ccf-4412-a9e6-0868f92d4e68"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eeb7ee-74d4-4189-bd3a-3e642670e591",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1750953054183,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "55eeb7ee-74d4-4189-bd3a-3e642670e591",
    "outputId": "e40abef7-64f9-489b-f9f9-edd55db58589"
   },
   "outputs": [],
   "source": [
    "# train_losses2 = [min(x,1) for x in train_losses]\n",
    "# val_losses2 = [min(x,2) for x in val_losses]\n",
    "helper_functions.plot_metrics([(train_losses2, \"Train Loss\"), (val_losses2, \"Validation Loss\")], \"Loss\")\n",
    "helper_functions.plot_metrics([(train_recons, \"Train MSE-Scores\"), (val_recons, \"Validation MSE-Scores\")], \"MSE-Scores\")\n",
    "# train_kls2 = [min(x,7) for x in train_kls]\n",
    "# val_kls2 = [min(x,7) for x in val_kls]\n",
    "helper_functions.plot_metrics([(train_kls2, \"Train KL-Scores\"), (val_kls2, \"Validation KL-Scores\")], \"KL-Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0c60612-6d73-4b01-b394-995b4bea45ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Std of mu per latent dim:\n",
      "tensor([1.0984, 1.4006, 1.3363])\n",
      "\n",
      "Mean of logvar per latent dim:\n",
      "tensor([-3.4504, -3.8279, -2.5183])\n",
      "\n",
      "Std of logvar per latent dim:\n",
      "tensor([1.1129, 1.1008, 0.4396])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "latents_mu = []\n",
    "latents_logvar = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        x = batch[0].to(device).float()\n",
    "        _, mu, logvar = model(x)\n",
    "        latents_mu.append(mu.cpu())\n",
    "        latents_logvar.append(logvar.cpu())\n",
    "\n",
    "mu_all = torch.cat(latents_mu, dim=0)           # shape: (n_samples, latent_dim)\n",
    "logvar_all = torch.cat(latents_logvar, dim=0)   # shape: (n_samples, latent_dim)\n",
    "\n",
    "# Statistics\n",
    "mu_std = mu_all.std(dim=0)\n",
    "logvar_mean = logvar_all.mean(dim=0)\n",
    "logvar_std = logvar_all.std(dim=0)\n",
    "\n",
    "print(\"Std of mu per latent dim:\")\n",
    "print(mu_std)\n",
    "\n",
    "print(\"\\nMean of logvar per latent dim:\")\n",
    "print(logvar_mean)\n",
    "\n",
    "print(\"\\nStd of logvar per latent dim:\")\n",
    "print(logvar_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0900f0b-2057-4fc5-b583-a0a64b532819",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"VCAE.pth\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7749099,
     "sourceId": 12294777,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7749153,
     "sourceId": 12301699,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 387008,
     "modelInstanceId": 366113,
     "sourceId": 451267,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
