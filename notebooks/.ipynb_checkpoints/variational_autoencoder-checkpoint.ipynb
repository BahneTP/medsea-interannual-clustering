{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4951ac0e-4f7b-4c6d-87e8-fa212d1750a9",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "This notebook makes use of the **Autoencoder**, which is used to reduce the dimensionality of our dataset in a non-linear way. Furthermore, we then apply **k-means Clustering** as in our last notebook in our new created **Latent Space** in lower dimension. We do so, to get rid of less important variables and achieve a better Clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b2f90b1-6826-4a2b-896b-f0f59aa9999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cartopy xarray matplotlib netCDF4 torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e3c815e-a7d9-46d6-a45d-c64b8ed20a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper_functions\n",
    "import importlib\n",
    "from ipywidgets import FloatSlider\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from ipywidgets import interact, IntSlider\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec2175a-a4a6-4443-89a4-c30147743c2e",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "180adc48-cb34-4e24-9730-b7ce1456550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(\"/home/jovyan/spatiotemporal-mining-medsea/data/medsea_daily_depth_47.nc\")\n",
    "ds = ds.sel(time=slice(\"2000-01-01\", \"2002-12-31\"))\n",
    "\n",
    "def preprocessing(features: list, depths: list, dim):\n",
    "    \"\"\"\n",
    "    feature: List of features to combine. \"sol\" for Salinity or \"temperature for Temperature.\n",
    "    depth: \"Any value from 0 to 1062, but it will assign the closest existing.\"\n",
    "\n",
    "    returns a stacked array of the \"feature\".\n",
    "    \"\"\"\n",
    "\n",
    "    def standardize(group):\n",
    "        m = group.mean(\"time\")\n",
    "        s = group.std(\"time\")\n",
    "        return (group - m) / s\n",
    "\n",
    "    feature_vectors = []\n",
    "    for feature in features:\n",
    "\n",
    "        for depth in depths:\n",
    "            data = ds[feature].sel(depth=depth, method=\"nearest\")\n",
    "            data = data.assign_coords(month=data[\"time\"].dt.day)\n",
    "        \n",
    "            z = data.groupby(\"month\").apply(standardize)\n",
    "            z = z.stack(location=(\"latitude\", \"longitude\"))\n",
    "            feature_vectors.append(z)\n",
    "    \n",
    "    z_concat = xr.concat(feature_vectors, dim=dim)\n",
    "    z_concat = z_concat.dropna(dim=\"location\", how=\"any\")\n",
    "\n",
    "    return z_concat  # → xarray.DataArray mit dims: (\"time\", \"location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c45eb1-fe33-437d-bb7b-91c8b1a76a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40657\n"
     ]
    }
   ],
   "source": [
    "z_temp = preprocessing([\"thetao\"], [50], \"location\")\n",
    "X = z_temp.values\n",
    "input_dimension = X.shape[1]\n",
    "print(X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11779e19-1403-4924-b480-f5457daaee2e",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36cff7de-fbe6-4edb-9c94-5fead6c7a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype(np.float32)\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=27)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(torch.from_numpy(X_train)), batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(torch.from_numpy(X_test)), batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed51ed5-9af0-42ad-a1a9-9d9b0b5edfe7",
   "metadata": {},
   "source": [
    "## The Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b14a26b5-80b3-48f8-850e-df17ee2fdae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        # Shared encoder backbone\n",
    "        self.encoder_shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # Separate layers for mean and log-variance\n",
    "        self.mu_layer = nn.Linear(64, latent_dim)\n",
    "        self.logvar_layer = nn.Linear(64, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, input_dim),\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder_shared(x)\n",
    "        mu = self.mu_layer(h)\n",
    "        logvar = self.logvar_layer(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f29428-7c81-4a80-a415-c051c92d86cd",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "439fbcc1-fb73-464f-b771-4b94041ff2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x_recon, x, mu, logvar, beta=1e-3):\n",
    "    recon_loss = F.mse_loss(x_recon, x, reduction='mean')\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
    "    total_loss = recon_loss + beta * kl_div\n",
    "    return total_loss, recon_loss.item(), kl_div.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f426540-b690-433f-9359-a00113b5a2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beta(epoch, total_epochs, max_beta=1e-1):\n",
    "    return max_beta * (epoch / total_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3135c56e-a342-4273-a344-db6ccb56a33e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8b3e058-34ba-439d-aeda-aa0a21e6dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs: int, kl_beta=1e-3):\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        kl_beta = get_beta(epoch, num_epochs) if kl_beta == -1 else kl_beta\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        # kl_beta = min(1e-3, epoch / 10 * 1e-4)\n",
    "        for batch in train_loader:\n",
    "            x = batch[0].to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            x_recon, mu, logvar = model(x)\n",
    "            loss, recon, kl = vae_loss(x_recon, x, mu, logvar, beta=kl_beta)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item() * x.size(0)\n",
    "    \n",
    "        train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "    \n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                x = batch[0].to(device).float()\n",
    "                x_recon, mu, logvar = model(x)\n",
    "                loss, recon, kl = vae_loss(x_recon, x, mu, logvar, beta=kl_beta)\n",
    "\n",
    "                running_val_loss += loss.item() * x.size(0)\n",
    "    \n",
    "        val_loss = running_val_loss / len(test_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "    \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Recon: {recon:.4f} | KL: {kl:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248c2819-ad9d-4ea5-96fc-1e5b6767a919",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c5c9d9-3fee-4276-892e-9456e006205c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using device: cuda\n",
      "Epoch 1/10 | Train Loss: 8.1143 | Val Loss: 1.1038 | Recon: 0.9085 | KL: 195.2715\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Training using device: {device}')\n",
    "\n",
    "model = VariationalAutoencoder(input_dim=input_dimension, latent_dim=9).float().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses = train(num_epochs,kl_beta=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68549efe-8ccf-4412-a9e6-0868f92d4e68",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eeb7ee-74d4-4189-bd3a-3e642670e591",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(train_losses, label=\"Train Loss\", marker='o')\n",
    "plt.plot(val_losses, label=\"Validation Loss\", marker='x')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712d21eb-5ce1-4f2e-906c-fcebde37cbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "latents_mu = []\n",
    "latents_logvar = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x = batch[0].to(device).float()\n",
    "        _, mu, logvar = model(x)\n",
    "        latents_mu.append(mu.cpu())\n",
    "        latents_logvar.append(logvar.cpu())\n",
    "\n",
    "# Alles zusammenfügen\n",
    "mu_all = torch.cat(latents_mu, dim=0)           # shape: (n_samples, latent_dim)\n",
    "logvar_all = torch.cat(latents_logvar, dim=0)   # shape: (n_samples, latent_dim)\n",
    "\n",
    "# Statistiken\n",
    "mu_std = mu_all.std(dim=0)\n",
    "logvar_mean = logvar_all.mean(dim=0)\n",
    "logvar_std = logvar_all.std(dim=0)\n",
    "\n",
    "# Ausgabe\n",
    "print(\"Std of mu per latent dim:\")\n",
    "print(mu_std)\n",
    "\n",
    "print(\"\\nMean of logvar per latent dim:\")\n",
    "print(logvar_mean)\n",
    "\n",
    "print(\"\\nStd of logvar per latent dim:\")\n",
    "print(logvar_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f99179-f7b6-455d-94f4-1b358fd67b74",
   "metadata": {},
   "source": [
    "## Clustering with K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6fa6b4-85e4-46e3-be6a-37327cbd0823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import torch\n",
    "\n",
    "def get_latents_and_cluster(model, X, n_clusters=5, batch_size=512, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Computes the latent representations (mu) for all inputs and clusters them using KMeans.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained VariationalAutoencoder\n",
    "    - X: Input data as a numpy array or torch.Tensor of shape (n_samples, input_dim)\n",
    "    - n_clusters: Number of clusters for KMeans\n",
    "    - batch_size: Batch size for efficient computation\n",
    "    - device: Device to run the model on (\"cpu\" or \"cuda\")\n",
    "\n",
    "    Returns:\n",
    "    - latents: Tensor of shape (n_samples, latent_dim)\n",
    "    - cluster_labels: Numpy array of shape (n_samples,) with cluster assignments\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    if isinstance(X, torch.Tensor):\n",
    "        X_tensor = X\n",
    "    else:\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "    X_tensor = X_tensor.to(device)\n",
    "    latents = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X_tensor), batch_size):\n",
    "            batch = X_tensor[i:i+batch_size]\n",
    "            h = model.encoder_shared(batch)\n",
    "            mu = model.mu_layer(h)\n",
    "            latents.append(mu.cpu())\n",
    "\n",
    "    latents = torch.cat(latents, dim=0)  # (n_samples, latent_dim)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(latents.numpy())\n",
    "\n",
    "    return latents, cluster_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbedd128-90ff-4067-b68b-fbf76397bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents, labels= get_latents_and_cluster(model, X, n_clusters = 9, batch_size=16)\n",
    "labels+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d17114-c79f-41b0-be9c-d5ee555056c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Here we preprocess only the temperature since we want\n",
    "# to only cluster the temperature's z-scores in the plots.\"\"\"\n",
    "helper_functions.plot_cluster_timeline(z_temp, labels)\n",
    "\n",
    "z_temp = preprocessing([\"thetao\"], [50], \"location\") \n",
    "helper_functions.plot_average_cluster(z_temp, labels, -2, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
