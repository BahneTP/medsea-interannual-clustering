{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4951ac0e-4f7b-4c6d-87e8-fa212d1750a9",
   "metadata": {
    "editable": true,
    "id": "4951ac0e-4f7b-4c6d-87e8-fa212d1750a9",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Convolutional Autoencoder\n",
    "\n",
    "This notebook uses a **Convolutional Autoencoder**, which is a neural network designed to compress spatial data like maps into a lower-dimensional **latent space**, and then reconstruct it. \n",
    "\n",
    "In our case, the goal is to learn compact representations of the oceanographic maps while preserving important spatial patterns. This helps reduce noise, ignore missing or irrelevant regions (thanks to the masking), and makes it easier to apply further analysis — for example, clustering similar patterns in the compressed space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b2f90b1-6826-4a2b-896b-f0f59aa9999c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 110274,
     "status": "ok",
     "timestamp": 1750955287591,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "4b2f90b1-6826-4a2b-896b-f0f59aa9999c",
    "outputId": "4dec4adf-f65f-4eea-9962-5fa5887df0ec"
   },
   "outputs": [],
   "source": [
    "!pip install -q -r ../../requirements.txt &> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a9a3b7-de75-47f5-8e47-612a35397ee0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e3c815e-a7d9-46d6-a45d-c64b8ed20a69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1750955461797,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "7e3c815e-a7d9-46d6-a45d-c64b8ed20a69",
    "outputId": "c8c3bc6e-81b3-473b-e58c-d53fa46ceb8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'helper_functions' from '/home/jovyan/spatiotemporal-mining-medsea/information_filtering/trend_removal/models/../../helper_functions.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "from torchinfo import summary\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import helper_functions\n",
    "import importlib\n",
    "importlib.reload(helper_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c7cb91c-8458-401b-9d69-1858f6be5b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 27\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f10a58f-d84f-4be4-abeb-478f20b1b76f",
   "metadata": {},
   "source": [
    "Since our ConvNet works with masked map data, it naturally produces some NaNs — that's expected behavior. To avoid flooding the output with warnings, we've disabled them here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec2175a-a4a6-4443-89a4-c30147743c2e",
   "metadata": {
    "id": "cec2175a-a4a6-4443-89a4-c30147743c2e"
   },
   "source": [
    "## Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c45eb1-fe33-437d-bb7b-91c8b1a76a99",
   "metadata": {
    "executionInfo": {
     "elapsed": 2945,
     "status": "ok",
     "timestamp": 1750955304911,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "25c45eb1-fe33-437d-bb7b-91c8b1a76a99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20554.545012 MB\n"
     ]
    }
   ],
   "source": [
    "ds = xr.open_dataset(\"/home/jovyan/spatiotemporal-mining-medsea/data/medsea.nc\")\n",
    "print(ds.nbytes / 1e6, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffe1213c-9dee-4d4e-91d3-012c3db1899c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "executionInfo": {
     "elapsed": 92667,
     "status": "ok",
     "timestamp": 1750955397579,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "ffe1213c-9dee-4d4e-91d3-012c3db1899c",
    "outputId": "5e8a58d3-c8db-4111-ec0e-9396fe41a514",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([342, 6, 203, 514])\n"
     ]
    }
   ],
   "source": [
    "X_np, M_np = helper_functions.preprocessing_conv(ds, [\"thetao\", \"so\"], [50, 300, 1000], True, 1)\n",
    "X = torch.tensor(X_np)  # (B, C, H, W)\n",
    "M = torch.tensor(M_np)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4956a2-917d-4d60-afc2-410b41767582",
   "metadata": {},
   "source": [
    "Again, we include all depths and features for reconstruction. The difference in our ConvNet is that these features are not simply concatenated, but stacked as channels — similar to how RGB channels work in images. This allows the network to capture spatial relationships between features and depths more effectively, which might be important in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11779e19-1403-4924-b480-f5457daaee2e",
   "metadata": {
    "id": "11779e19-1403-4924-b480-f5457daaee2e"
   },
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "974f3cdc-1c88-4f71-88ee-bb7d3fe13362",
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1750955397581,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "974f3cdc-1c88-4f71-88ee-bb7d3fe13362"
   },
   "outputs": [],
   "source": [
    "class MaskedDataset(Dataset):\n",
    "    def __init__(self, X, M):\n",
    "        self.X = X\n",
    "        self.M = M\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.M[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36cff7de-fbe6-4edb-9c94-5fead6c7a13a",
   "metadata": {
    "executionInfo": {
     "elapsed": 874,
     "status": "ok",
     "timestamp": 1750955398447,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "36cff7de-fbe6-4edb-9c94-5fead6c7a13a"
   },
   "outputs": [],
   "source": [
    "full_dataset = MaskedDataset(X_np, M_np)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "generator = torch.Generator().manual_seed(SEED)\n",
    "train_set, val_set = torch.utils.data.random_split(\n",
    "    full_dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=generator\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(val_set, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed51ed5-9af0-42ad-a1a9-9d9b0b5edfe7",
   "metadata": {
    "id": "4ed51ed5-9af0-42ad-a1a9-9d9b0b5edfe7"
   },
   "source": [
    "## The Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "BmPchZzRmXID",
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1750955534808,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "BmPchZzRmXID"
   },
   "outputs": [],
   "source": [
    "class CAE(nn.Module):\n",
    "    def __init__(self, in_channels, latent_dim=3, dropout_p=0.2, channels=[32, 64, 128], input_shape=(203, 514)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.channels = channels\n",
    "\n",
    "        encoder_layers = []\n",
    "        prev_channels = in_channels\n",
    "        h, w = input_shape\n",
    "\n",
    "        for ch in channels:\n",
    "            encoder_layers += [\n",
    "                nn.Conv2d(prev_channels, ch, kernel_size=3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(ch),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout2d(p=dropout_p),\n",
    "            ]\n",
    "            prev_channels = ch\n",
    "            h = math.floor((h + 2 * 1 - 3) / 2 + 1)  # Conv2d output size formula\n",
    "            w = math.floor((w + 2 * 1 - 3) / 2 + 1)\n",
    "\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        self.unflatten_shape = (channels[-1], h, w)\n",
    "\n",
    "        flat_dim = channels[-1] * h * w\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_enc = nn.Sequential(\n",
    "            nn.Linear(flat_dim, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(1024, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(64, latent_dim)\n",
    "        )\n",
    "        self.fc_dec = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(64, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(1024, flat_dim)\n",
    "        )\n",
    "\n",
    "        decoder_layers = []\n",
    "        rev_channels = list(reversed(channels))\n",
    "        for i in range(len(rev_channels) - 1):\n",
    "            decoder_layers += [\n",
    "                nn.ConvTranspose2d(rev_channels[i], rev_channels[i + 1], kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(rev_channels[i + 1]),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout2d(p=dropout_p),\n",
    "            ]\n",
    "\n",
    "        decoder_layers += [\n",
    "            nn.ConvTranspose2d(rev_channels[-1], in_channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        ]\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        if mask is not None:\n",
    "            x = x * mask\n",
    "        x = self.encoder(x)\n",
    "        z = self.fc_enc(self.flatten(x))\n",
    "        x = self.fc_dec(z)\n",
    "        x = x.view(x.size(0), *self.unflatten_shape)\n",
    "        x = self.decoder(x)\n",
    "        return x[:, :, :self.input_shape[0], :self.input_shape[1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd997601-9a5d-4a69-a879-fe1c9936e87b",
   "metadata": {},
   "source": [
    "### Convolutional Autoencoder\n",
    "\n",
    "We use a simple convolutional autoencoder that compresses the input maps into a small latent vector and reconstructs them back. The encoder reduces spatial resolution through convolutional layers, while the decoder upsamples the data back using transposed convolutions.\n",
    "\n",
    "I experimented with different numbers of layers, kernel sizes, and dropout values. The current setup gave the best trade-off between reconstruction quality and training stability.\n",
    "\n",
    "\n",
    "### Hard Masking\n",
    "- Since many values in the input maps are invalid (e.g. land areas), we apply a **hard mask** before feeding the data into the encoder.\n",
    "- This mask zeroes out irrelevant values, so the model only learns from valid oceanic regions.\n",
    "- During training, the same mask is applied to the loss function to ensure the model is not penalized for errors in masked-out areas.\n",
    "- This is essential for learning robust spatial patterns without being misled by missing or irrelevant data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f29428-7c81-4a80-a415-c051c92d86cd",
   "metadata": {
    "id": "f9f29428-7c81-4a80-a415-c051c92d86cd"
   },
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f36d7cfa-8d22-41f0-a1b0-fb2c31ac8f5e",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1750955398551,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "f36d7cfa-8d22-41f0-a1b0-fb2c31ac8f5e"
   },
   "outputs": [],
   "source": [
    "def masked_mse(x_recon, x_true, mask):\n",
    "    loss = ((x_recon - x_true) ** 2) * mask\n",
    "    return loss.sum() / mask.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696eba88-be08-4790-a145-0a3ddeeffaa2",
   "metadata": {
    "id": "696eba88-be08-4790-a145-0a3ddeeffaa2"
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8b3e058-34ba-439d-aeda-aa0a21e6dd5e",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1750955398556,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "c8b3e058-34ba-439d-aeda-aa0a21e6dd5e"
   },
   "outputs": [],
   "source": [
    "def train(num_epochs: int):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "\n",
    "        for x, mask in train_loader:\n",
    "            x = x.to(device)\n",
    "            mask = mask.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            x_recon = model(x, mask=mask)\n",
    "            loss = masked_mse(x_recon, x, mask)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item() * x.size(0)\n",
    "\n",
    "        train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, mask in test_loader:\n",
    "                x = x.to(device)\n",
    "                mask = mask.to(device)\n",
    "                x_recon = model(x, mask=mask)\n",
    "                loss = masked_mse(x_recon, x, mask)\n",
    "                running_val_loss += loss.item() * x.size(0)\n",
    "\n",
    "        val_loss = running_val_loss / len(test_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd29feb4-1f80-400e-b1f7-a41f47fcace9",
   "metadata": {
    "id": "bd29feb4-1f80-400e-b1f7-a41f47fcace9"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22fecc03-c5a1-49d2-87d0-9b490e8ff0c2",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1750955398545,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "22fecc03-c5a1-49d2-87d0-9b490e8ff0c2"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36mleCQi1wa1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "executionInfo": {
     "elapsed": 224,
     "status": "error",
     "timestamp": 1750955678818,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "36mleCQi1wa1",
    "outputId": "618c29cd-1083-48b1-b84a-7a9790043df4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CAE                                      [1, 6, 203, 514]          --\n",
       "├─Sequential: 1-1                        [1, 1024, 4, 9]           --\n",
       "│    └─Conv2d: 2-1                       [1, 32, 102, 257]         1,760\n",
       "│    └─InstanceNorm2d: 2-2               [1, 32, 102, 257]         --\n",
       "│    └─LeakyReLU: 2-3                    [1, 32, 102, 257]         --\n",
       "│    └─Dropout2d: 2-4                    [1, 32, 102, 257]         --\n",
       "│    └─Conv2d: 2-5                       [1, 64, 51, 129]          18,496\n",
       "│    └─InstanceNorm2d: 2-6               [1, 64, 51, 129]          --\n",
       "│    └─LeakyReLU: 2-7                    [1, 64, 51, 129]          --\n",
       "│    └─Dropout2d: 2-8                    [1, 64, 51, 129]          --\n",
       "│    └─Conv2d: 2-9                       [1, 128, 26, 65]          73,856\n",
       "│    └─InstanceNorm2d: 2-10              [1, 128, 26, 65]          --\n",
       "│    └─LeakyReLU: 2-11                   [1, 128, 26, 65]          --\n",
       "│    └─Dropout2d: 2-12                   [1, 128, 26, 65]          --\n",
       "│    └─Conv2d: 2-13                      [1, 256, 13, 33]          295,168\n",
       "│    └─InstanceNorm2d: 2-14              [1, 256, 13, 33]          --\n",
       "│    └─LeakyReLU: 2-15                   [1, 256, 13, 33]          --\n",
       "│    └─Dropout2d: 2-16                   [1, 256, 13, 33]          --\n",
       "│    └─Conv2d: 2-17                      [1, 512, 7, 17]           1,180,160\n",
       "│    └─InstanceNorm2d: 2-18              [1, 512, 7, 17]           --\n",
       "│    └─LeakyReLU: 2-19                   [1, 512, 7, 17]           --\n",
       "│    └─Dropout2d: 2-20                   [1, 512, 7, 17]           --\n",
       "│    └─Conv2d: 2-21                      [1, 1024, 4, 9]           4,719,616\n",
       "│    └─InstanceNorm2d: 2-22              [1, 1024, 4, 9]           --\n",
       "│    └─LeakyReLU: 2-23                   [1, 1024, 4, 9]           --\n",
       "│    └─Dropout2d: 2-24                   [1, 1024, 4, 9]           --\n",
       "├─Flatten: 1-2                           [1, 36864]                --\n",
       "├─Sequential: 1-3                        [1, 3]                    --\n",
       "│    └─Linear: 2-25                      [1, 1024]                 37,749,760\n",
       "│    └─LeakyReLU: 2-26                   [1, 1024]                 --\n",
       "│    └─Dropout: 2-27                     [1, 1024]                 --\n",
       "│    └─Linear: 2-28                      [1, 64]                   65,600\n",
       "│    └─LeakyReLU: 2-29                   [1, 64]                   --\n",
       "│    └─Dropout: 2-30                     [1, 64]                   --\n",
       "│    └─Linear: 2-31                      [1, 3]                    195\n",
       "├─Sequential: 1-4                        [1, 36864]                --\n",
       "│    └─Linear: 2-32                      [1, 64]                   256\n",
       "│    └─LeakyReLU: 2-33                   [1, 64]                   --\n",
       "│    └─Dropout: 2-34                     [1, 64]                   --\n",
       "│    └─Linear: 2-35                      [1, 1024]                 66,560\n",
       "│    └─LeakyReLU: 2-36                   [1, 1024]                 --\n",
       "│    └─Dropout: 2-37                     [1, 1024]                 --\n",
       "│    └─Linear: 2-38                      [1, 36864]                37,785,600\n",
       "├─Sequential: 1-5                        [1, 6, 256, 576]          --\n",
       "│    └─ConvTranspose2d: 2-39             [1, 512, 8, 18]           4,719,104\n",
       "│    └─InstanceNorm2d: 2-40              [1, 512, 8, 18]           --\n",
       "│    └─LeakyReLU: 2-41                   [1, 512, 8, 18]           --\n",
       "│    └─Dropout2d: 2-42                   [1, 512, 8, 18]           --\n",
       "│    └─ConvTranspose2d: 2-43             [1, 256, 16, 36]          1,179,904\n",
       "│    └─InstanceNorm2d: 2-44              [1, 256, 16, 36]          --\n",
       "│    └─LeakyReLU: 2-45                   [1, 256, 16, 36]          --\n",
       "│    └─Dropout2d: 2-46                   [1, 256, 16, 36]          --\n",
       "│    └─ConvTranspose2d: 2-47             [1, 128, 32, 72]          295,040\n",
       "│    └─InstanceNorm2d: 2-48              [1, 128, 32, 72]          --\n",
       "│    └─LeakyReLU: 2-49                   [1, 128, 32, 72]          --\n",
       "│    └─Dropout2d: 2-50                   [1, 128, 32, 72]          --\n",
       "│    └─ConvTranspose2d: 2-51             [1, 64, 64, 144]          73,792\n",
       "│    └─InstanceNorm2d: 2-52              [1, 64, 64, 144]          --\n",
       "│    └─LeakyReLU: 2-53                   [1, 64, 64, 144]          --\n",
       "│    └─Dropout2d: 2-54                   [1, 64, 64, 144]          --\n",
       "│    └─ConvTranspose2d: 2-55             [1, 32, 128, 288]         18,464\n",
       "│    └─InstanceNorm2d: 2-56              [1, 32, 128, 288]         --\n",
       "│    └─LeakyReLU: 2-57                   [1, 32, 128, 288]         --\n",
       "│    └─Dropout2d: 2-58                   [1, 32, 128, 288]         --\n",
       "│    └─ConvTranspose2d: 2-59             [1, 6, 256, 576]          1,734\n",
       "==========================================================================================\n",
       "Total params: 88,245,065\n",
       "Trainable params: 88,245,065\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 4.46\n",
       "==========================================================================================\n",
       "Input size (MB): 2.50\n",
       "Forward/backward pass size (MB): 39.15\n",
       "Params size (MB): 352.98\n",
       "Estimated Total Size (MB): 394.63\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# torch.cuda.reset_peak_memory_stats()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CAE(in_channels=X_np.shape[1], latent_dim=3, dropout_p=0.2, channels= [32, 64, 128, 256, 512, 1024]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0004)\n",
    "\n",
    "summary(model, input_size=(1, X_np.shape[1], 203, 514))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PUQ6KPZs2CZL",
   "metadata": {
    "editable": true,
    "id": "PUQ6KPZs2CZL",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/800 | Train Loss: 1.0037 | Val Loss: 0.9918\n",
      "Epoch 20/800 | Train Loss: 1.0025 | Val Loss: 0.9919\n",
      "Epoch 30/800 | Train Loss: 1.0027 | Val Loss: 0.9920\n",
      "Epoch 40/800 | Train Loss: 0.9998 | Val Loss: 0.9894\n",
      "Epoch 50/800 | Train Loss: 0.9913 | Val Loss: 0.9788\n",
      "Epoch 60/800 | Train Loss: 0.9481 | Val Loss: 0.9297\n",
      "Epoch 70/800 | Train Loss: 0.9084 | Val Loss: 0.8799\n",
      "Epoch 80/800 | Train Loss: 0.8749 | Val Loss: 0.8556\n",
      "Epoch 90/800 | Train Loss: 0.8452 | Val Loss: 0.8178\n",
      "Epoch 100/800 | Train Loss: 0.8002 | Val Loss: 0.7944\n",
      "Epoch 110/800 | Train Loss: 0.7626 | Val Loss: 0.7550\n",
      "Epoch 120/800 | Train Loss: 0.7099 | Val Loss: 0.7226\n",
      "Epoch 130/800 | Train Loss: 0.6836 | Val Loss: 0.7044\n",
      "Epoch 140/800 | Train Loss: 0.6611 | Val Loss: 0.6770\n",
      "Epoch 150/800 | Train Loss: 0.6389 | Val Loss: 0.6579\n",
      "Epoch 160/800 | Train Loss: 0.6113 | Val Loss: 0.6327\n",
      "Epoch 170/800 | Train Loss: 0.5943 | Val Loss: 0.6141\n",
      "Epoch 180/800 | Train Loss: 0.5838 | Val Loss: 0.6054\n",
      "Epoch 190/800 | Train Loss: 0.5747 | Val Loss: 0.5926\n",
      "Epoch 200/800 | Train Loss: 0.5626 | Val Loss: 0.5940\n",
      "Epoch 210/800 | Train Loss: 0.5529 | Val Loss: 0.5866\n",
      "Epoch 220/800 | Train Loss: 0.5475 | Val Loss: 0.5789\n",
      "Epoch 230/800 | Train Loss: 0.5363 | Val Loss: 0.5754\n",
      "Epoch 240/800 | Train Loss: 0.5284 | Val Loss: 0.5628\n",
      "Epoch 250/800 | Train Loss: 0.5220 | Val Loss: 0.5610\n",
      "Epoch 260/800 | Train Loss: 0.5237 | Val Loss: 0.5693\n",
      "Epoch 270/800 | Train Loss: 0.5115 | Val Loss: 0.5465\n",
      "Epoch 280/800 | Train Loss: 0.5051 | Val Loss: 0.5479\n",
      "Epoch 290/800 | Train Loss: 0.5033 | Val Loss: 0.5549\n",
      "Epoch 300/800 | Train Loss: 0.4992 | Val Loss: 0.5511\n",
      "Epoch 310/800 | Train Loss: 0.5146 | Val Loss: 0.5450\n",
      "Epoch 320/800 | Train Loss: 0.4906 | Val Loss: 0.5331\n",
      "Epoch 330/800 | Train Loss: 0.4926 | Val Loss: 0.5362\n",
      "Epoch 340/800 | Train Loss: 0.4950 | Val Loss: 0.5458\n",
      "Epoch 350/800 | Train Loss: 0.4755 | Val Loss: 0.5681\n",
      "Epoch 360/800 | Train Loss: 0.4968 | Val Loss: 0.5325\n",
      "Epoch 370/800 | Train Loss: 0.4698 | Val Loss: 0.5332\n",
      "Epoch 380/800 | Train Loss: 0.4693 | Val Loss: 0.5210\n",
      "Epoch 390/800 | Train Loss: 0.4629 | Val Loss: 0.5057\n",
      "Epoch 400/800 | Train Loss: 0.4622 | Val Loss: 0.5131\n",
      "Epoch 410/800 | Train Loss: 0.4495 | Val Loss: 0.5099\n",
      "Epoch 420/800 | Train Loss: 0.4568 | Val Loss: 0.5042\n",
      "Epoch 430/800 | Train Loss: 0.4514 | Val Loss: 0.5085\n",
      "Epoch 440/800 | Train Loss: 0.4396 | Val Loss: 0.5107\n",
      "Epoch 450/800 | Train Loss: 0.4310 | Val Loss: 0.5157\n",
      "Epoch 460/800 | Train Loss: 0.4425 | Val Loss: 0.5172\n",
      "Epoch 470/800 | Train Loss: 0.4526 | Val Loss: 0.5045\n",
      "Epoch 480/800 | Train Loss: 0.4530 | Val Loss: 0.5099\n",
      "Epoch 490/800 | Train Loss: 0.4267 | Val Loss: 0.4969\n",
      "Epoch 500/800 | Train Loss: 0.4400 | Val Loss: 0.5151\n",
      "Epoch 510/800 | Train Loss: 0.4425 | Val Loss: 0.4970\n",
      "Epoch 520/800 | Train Loss: 0.4286 | Val Loss: 0.5125\n",
      "Epoch 530/800 | Train Loss: 0.4253 | Val Loss: 0.4900\n",
      "Epoch 540/800 | Train Loss: 0.4286 | Val Loss: 0.4998\n",
      "Epoch 550/800 | Train Loss: 0.4192 | Val Loss: 0.4880\n",
      "Epoch 560/800 | Train Loss: 0.4179 | Val Loss: 0.4964\n",
      "Epoch 570/800 | Train Loss: 0.4047 | Val Loss: 0.4911\n",
      "Epoch 580/800 | Train Loss: 0.4114 | Val Loss: 0.4909\n",
      "Epoch 590/800 | Train Loss: 0.4045 | Val Loss: 0.4877\n",
      "Epoch 600/800 | Train Loss: 0.4040 | Val Loss: 0.4814\n",
      "Epoch 610/800 | Train Loss: 0.4078 | Val Loss: 0.4841\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 800\n",
    "train_losses, val_losses = train(num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68549efe-8ccf-4412-a9e6-0868f92d4e68",
   "metadata": {
    "id": "68549efe-8ccf-4412-a9e6-0868f92d4e68"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b7c521-adda-4880-a83d-a9a10f780db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_functions.plot_metrics([(train_losses, \"Train Loss\"), (val_losses, \"Validation Loss\")], \"Loss (MSE)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0900f0b-2057-4fc5-b583-a0a64b532819",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"CAE.pth\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7749099,
     "sourceId": 12294777,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7749153,
     "sourceId": 12301699,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 387008,
     "modelInstanceId": 366113,
     "sourceId": 451267,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
