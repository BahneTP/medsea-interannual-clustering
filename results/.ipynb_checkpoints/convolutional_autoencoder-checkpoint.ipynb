{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4951ac0e-4f7b-4c6d-87e8-fa212d1750a9",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoder\n",
    "This notebook makes use of the **Autoencoder**, which is used to reduce the dimensionality of our dataset in a non-linear way. Furthermore, we then apply **k-means Clustering** as in our last notebook in our new created **Latent Space** in lower dimension. We do so, to get rid of less important variables and achieve a better Clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b2f90b1-6826-4a2b-896b-f0f59aa9999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cartopy xarray matplotlib netCDF4 torch torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e3c815e-a7d9-46d6-a45d-c64b8ed20a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'helper_functions' from '/home/jovyan/spatiotemporal-mining-medsea/notebooks/helper_functions.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import helper_functions\n",
    "import importlib\n",
    "from ipywidgets import FloatSlider\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from ipywidgets import interact, IntSlider\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "importlib.reload(helper_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec2175a-a4a6-4443-89a4-c30147743c2e",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25c45eb1-fe33-437d-bb7b-91c8b1a76a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(\"/home/jovyan/spatiotemporal-mining-medsea/data/medsea.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe1213c-9dee-4d4e-91d3-012c3db1899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np, M_np = helper_functions.preprocessing_conv(ds, [\"thetao\", \"so\"], [50, 300, 1000], 3)\n",
    "X = torch.tensor(X_np)  # (B, C, H, W)\n",
    "M = torch.tensor(M_np)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11779e19-1403-4924-b480-f5457daaee2e",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "974f3cdc-1c88-4f71-88ee-bb7d3fe13362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MaskedDataset(Dataset):\n",
    "    def __init__(self, X, M):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.M = torch.tensor(M, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.M[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36cff7de-fbe6-4edb-9c94-5fead6c7a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = MaskedDataset(X_np, M_np)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_set, val_set = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(val_set, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed51ed5-9af0-42ad-a1a9-9d9b0b5edfe7",
   "metadata": {},
   "source": [
    "## The Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39012ba2-ca5b-4391-9b2a-d62d04d2e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CAE(nn.Module):\n",
    "    def __init__(self, in_channels, latent_dim=3, dropout_p=0.2, channels=[32,64,128]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 3, stride=2, padding=1),   # → 102×257\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout2d(p=dropout_p),                             # spatial dropout\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),             # → 51×129\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout2d(p=dropout_p),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),            # → 26×65\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout2d(p=dropout_p),\n",
    "        )\n",
    "\n",
    "        self.unflatten_shape = (128, 26, 65)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_enc = nn.Sequential(\n",
    "            nn.Linear(128 * 26 * 65, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(64, latent_dim),\n",
    "        )\n",
    "        self.fc_dec = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(64, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(256, 128 * 26 * 65),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout2d(p=dropout_p),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout2d(p=dropout_p),\n",
    "\n",
    "            nn.ConvTranspose2d(32, in_channels, 3, stride=2, padding=1, output_padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        z = self.fc_enc(self.flatten(x))\n",
    "        x = self.fc_dec(z)\n",
    "        x = x.view(x.size(0), *self.unflatten_shape)\n",
    "        x = self.decoder(x)\n",
    "        return x[:, :, :203, :514]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfafbd15-0b05-4ef8-9bdb-897ebe9c1d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class CAE(nn.Module):\n",
    "#     def __init__(self, in_channels, latent_dim=3, dropout_p=0.1, channels=[32, 64, 128]):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Dynamischer Aufbau des Encoders\n",
    "#         encoder_layers = []\n",
    "#         prev_channels = in_channels\n",
    "#         for ch in channels:\n",
    "#             encoder_layers.extend([\n",
    "#                 nn.Conv2d(prev_channels, ch, 3, stride=2, padding=1),\n",
    "#                 nn.InstanceNorm2d(ch),\n",
    "#                 nn.LeakyReLU(),\n",
    "#                 nn.Dropout2d(p=dropout_p)\n",
    "#             ])\n",
    "#             prev_channels = ch\n",
    "#         self.encoder = nn.Sequential(*encoder_layers)\n",
    "\n",
    "#         # Letzter Shape nach Encoding (du musst diese Werte selbst setzen je nach input size!)\n",
    "#         # Beispiel: 3 Pooling-Stufen bei Input 203x514 → 203//8 ≈ 25, 514//8 ≈ 64\n",
    "#         self.unflatten_shape = (channels[-1], 26, 65)\n",
    "#         flatten_dim = channels[-1] * self.unflatten_shape[1] * self.unflatten_shape[2]\n",
    "\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.fc_enc = nn.Sequential(\n",
    "#             nn.Linear(flatten_dim, 256),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Dropout(p=dropout_p),\n",
    "#             nn.Linear(256, 64),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Dropout(p=dropout_p),\n",
    "#             nn.Linear(64, latent_dim),\n",
    "#         )\n",
    "#         self.fc_dec = nn.Sequential(\n",
    "#             nn.Linear(latent_dim, 64),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Dropout(p=dropout_p),\n",
    "#             nn.Linear(64, 256),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Dropout(p=dropout_p),\n",
    "#             nn.Linear(256, flatten_dim),\n",
    "#         )\n",
    "\n",
    "#         # Dynamischer Decoder-Aufbau (inverse Reihenfolge)\n",
    "#         decoder_layers = []\n",
    "#         rev_channels = list(reversed(channels))\n",
    "#         for i in range(len(rev_channels) - 1):\n",
    "#             decoder_layers.extend([\n",
    "#                 nn.ConvTranspose2d(rev_channels[i], rev_channels[i+1], 3, stride=2, padding=1, output_padding=1),\n",
    "#                 nn.InstanceNorm2d(rev_channels[i+1]),\n",
    "#                 nn.LeakyReLU(),\n",
    "#                 nn.Dropout2d(p=dropout_p)\n",
    "#             ])\n",
    "#         decoder_layers.append(\n",
    "#             nn.ConvTranspose2d(rev_channels[-1], in_channels, 3, stride=2, padding=1, output_padding=1)\n",
    "#         )\n",
    "#         self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.encoder(x)\n",
    "#         z = self.fc_enc(self.flatten(x))\n",
    "#         x = self.fc_dec(z)\n",
    "#         x = x.view(x.size(0), *self.unflatten_shape)\n",
    "#         x = self.decoder(x)\n",
    "#         return x[:, :, :203, :514]  # optional crop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22fecc03-c5a1-49d2-87d0-9b490e8ff0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f29428-7c81-4a80-a415-c051c92d86cd",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f36d7cfa-8d22-41f0-a1b0-fb2c31ac8f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mse(x_recon, x_true, mask):\n",
    "    loss = ((x_recon - x_true) ** 2) * mask\n",
    "    return loss.sum() / mask.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696eba88-be08-4790-a145-0a3ddeeffaa2",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8b3e058-34ba-439d-aeda-aa0a21e6dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs: int):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "\n",
    "        for x, mask in train_loader:\n",
    "            x = x.to(device)\n",
    "            mask = mask.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            x_recon = model(x)\n",
    "            loss = masked_mse(x_recon, x, mask)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item() * x.size(0)\n",
    "\n",
    "        train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, mask in test_loader:\n",
    "                x = x.to(device)\n",
    "                mask = mask.to(device)\n",
    "                x_recon = model(x)\n",
    "                loss = masked_mse(x_recon, x, mask)\n",
    "                running_val_loss += loss.item() * x.size(0)\n",
    "\n",
    "        val_loss = running_val_loss / len(test_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd29feb4-1f80-400e-b1f7-a41f47fcace9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87c5c9d9-3fee-4276-892e-9456e006205c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CAE                                      [1, 6, 203, 514]          --\n",
       "├─Sequential: 1-1                        [1, 128, 26, 65]          --\n",
       "│    └─Conv2d: 2-1                       [1, 32, 102, 257]         1,760\n",
       "│    └─InstanceNorm2d: 2-2               [1, 32, 102, 257]         --\n",
       "│    └─LeakyReLU: 2-3                    [1, 32, 102, 257]         --\n",
       "│    └─Dropout2d: 2-4                    [1, 32, 102, 257]         --\n",
       "│    └─Conv2d: 2-5                       [1, 64, 51, 129]          18,496\n",
       "│    └─InstanceNorm2d: 2-6               [1, 64, 51, 129]          --\n",
       "│    └─LeakyReLU: 2-7                    [1, 64, 51, 129]          --\n",
       "│    └─Dropout2d: 2-8                    [1, 64, 51, 129]          --\n",
       "│    └─Conv2d: 2-9                       [1, 128, 26, 65]          73,856\n",
       "│    └─InstanceNorm2d: 2-10              [1, 128, 26, 65]          --\n",
       "│    └─LeakyReLU: 2-11                   [1, 128, 26, 65]          --\n",
       "│    └─Dropout2d: 2-12                   [1, 128, 26, 65]          --\n",
       "├─Flatten: 1-2                           [1, 216320]               --\n",
       "├─Sequential: 1-3                        [1, 3]                    --\n",
       "│    └─Linear: 2-13                      [1, 256]                  55,378,176\n",
       "│    └─LeakyReLU: 2-14                   [1, 256]                  --\n",
       "│    └─Dropout: 2-15                     [1, 256]                  --\n",
       "│    └─Linear: 2-16                      [1, 64]                   16,448\n",
       "│    └─LeakyReLU: 2-17                   [1, 64]                   --\n",
       "│    └─Dropout: 2-18                     [1, 64]                   --\n",
       "│    └─Linear: 2-19                      [1, 3]                    195\n",
       "├─Sequential: 1-4                        [1, 216320]               --\n",
       "│    └─Linear: 2-20                      [1, 64]                   256\n",
       "│    └─LeakyReLU: 2-21                   [1, 64]                   --\n",
       "│    └─Dropout: 2-22                     [1, 64]                   --\n",
       "│    └─Linear: 2-23                      [1, 256]                  16,640\n",
       "│    └─LeakyReLU: 2-24                   [1, 256]                  --\n",
       "│    └─Dropout: 2-25                     [1, 256]                  --\n",
       "│    └─Linear: 2-26                      [1, 216320]               55,594,240\n",
       "├─Sequential: 1-5                        [1, 6, 208, 520]          --\n",
       "│    └─ConvTranspose2d: 2-27             [1, 64, 52, 130]          73,792\n",
       "│    └─InstanceNorm2d: 2-28              [1, 64, 52, 130]          --\n",
       "│    └─LeakyReLU: 2-29                   [1, 64, 52, 130]          --\n",
       "│    └─Dropout2d: 2-30                   [1, 64, 52, 130]          --\n",
       "│    └─ConvTranspose2d: 2-31             [1, 32, 104, 260]         18,464\n",
       "│    └─InstanceNorm2d: 2-32              [1, 32, 104, 260]         --\n",
       "│    └─LeakyReLU: 2-33                   [1, 32, 104, 260]         --\n",
       "│    └─Dropout2d: 2-34                   [1, 32, 104, 260]         --\n",
       "│    └─ConvTranspose2d: 2-35             [1, 6, 208, 520]          1,734\n",
       "==========================================================================================\n",
       "Total params: 111,194,057\n",
       "Trainable params: 111,194,057\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.59\n",
       "==========================================================================================\n",
       "Input size (MB): 2.50\n",
       "Forward/backward pass size (MB): 29.12\n",
       "Params size (MB): 444.78\n",
       "Estimated Total Size (MB): 476.40\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CAE(in_channels=X_np.shape[1], latent_dim=3, dropout_p=0.1 ,channels=[32, 64, 128]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(model, input_size=(1, X_np.shape[1], 203, 514))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07ebe93-dfb5-4724-9d17-ee7c4cbbbe1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 1.0240 | Val Loss: 0.9954\n",
      "Epoch 2/100 | Train Loss: 1.0031 | Val Loss: 0.9942\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "train_losses, val_losses = train(num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68549efe-8ccf-4412-a9e6-0868f92d4e68",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eeb7ee-74d4-4189-bd3a-3e642670e591",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(train_losses[190:], label=\"Train Loss\", marker='o')\n",
    "plt.plot(val_losses[190:], label=\"Validation Loss\", marker='x')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fca60a-dc9b-4276-b7d2-b0dec85f4b07",
   "metadata": {},
   "source": [
    "## Rconstructing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078dae08-0258-436a-b2ae-9b176e3c45b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_recon_all = helper_functions.reconstruct_in_batches(X, model, device, batch_size=16)\n",
    "\n",
    "M_tensor = torch.tensor(M_np[:, :, :203, :514], dtype=torch.float32)\n",
    "valid_flat_mask = (M_tensor[0] > 0).reshape(-1).cpu().numpy()\n",
    "\n",
    "X_recon_all = helper_functions.reconstruction_to_vector_masked_positions(X_recon_all, valid_flat_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee451fc5-beb4-4f9f-b93e-7de9a3d1387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schritt 1 – Originale z-Werte stacken\n",
    "original_z = helper_functions.preprocessing(ds, [\"thetao\"], [50], \"location\")\n",
    "\n",
    "# Schritt 2 – Validitätsmaske vorbereiten\n",
    "M_mask = M_tensor[0].reshape(-1) > 0  # (C * H * W,)\n",
    "location_coords = original_z.location.values\n",
    "time_coords = original_z.time.values\n",
    "\n",
    "# Schritt 3 – CAE-Rekonstruktion erzeugen\n",
    "X_recon = helper_functions.reconstruct_in_batches(X, model, device, batch_size=16)\n",
    "\n",
    "# Schritt 4 – Nur gültige Positionen extrahieren\n",
    "flat_recon = helper_functions.reconstruction_to_vector_masked_positions(X_recon, M_mask)\n",
    "\n",
    "# Schritt 5 – Reconstructed z_stack erzeugen\n",
    "# reconstructed_z = build_z_stack(flat_recon, location_coords, time_coords)\n",
    "reconstructed_z = helper_functions.reconstructed_to_stack(ds, \"thetao\", 50, flat_recon)\n",
    "\n",
    "# Schritt 6 – Vergleich anzeigen\n",
    "helper_functions.plot_reconstruction_comparison(\n",
    "    z_stack_original=original_z,\n",
    "    z_stack_recon=reconstructed_z,\n",
    "    time_indices=[0, 15, 31, 190],\n",
    "    cmin=-2,\n",
    "    cmax=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f99179-f7b6-455d-94f4-1b358fd67b74",
   "metadata": {},
   "source": [
    "## Clustering with K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ae031-58a8-429a-861c-73c492b03156",
   "metadata": {},
   "source": [
    "### Clustering the reconstructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a61962-209e-4a2d-b7b1-b90039ed35a0",
   "metadata": {},
   "source": [
    "### Latent = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479b53d1-ba18-4b0f-816c-3c7447d70fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(helper_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4a73e2-4a76-42ad-b9b3-c151a41a7bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=9\n",
    "labels = helper_functions.apply_kmeans(X_recon_all, k)\n",
    "labels += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2114e47a-bae3-4032-b061-8316a1c13fd3",
   "metadata": {},
   "source": [
    "### Depth = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4867c-f575-4c1e-be8b-526fda51a41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_temp_50 = helper_functions.reconstructed_to_stack(ds, \"thetao\", 50, X_recon_all)\n",
    "\n",
    "helper_functions.plot_cluster_timeline(recon_temp_50, labels)\n",
    "helper_functions.plot_average_cluster(recon_temp_50, labels, -2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c9498c-c249-4b1b-b339-21983464a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_so_50 = helper_functions.reconstructed_to_stack(ds, \"so\", 50, X_recon_all)\n",
    "helper_functions.plot_average_cluster(recon_so_50, labels, -2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33092aea-8657-4891-b78f-b988625ef332",
   "metadata": {},
   "source": [
    "### Depth = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84525a60-dd18-4334-983b-8a378da7abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_temp_300 = helper_functions.reconstructed_to_stack(ds, \"thetao\", 300, X_recon_all)\n",
    "helper_functions.plot_average_cluster(recon_temp_300, labels, -2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e538a1d4-592a-4b14-971d-a96d86daf88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_so_300 = helper_functions.reconstructed_to_stack(ds, \"so\", 300, X_recon_all)\n",
    "helper_functions.plot_average_cluster(recon_so_300, labels, -2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4a9009-4933-4202-9dc1-ac065583b8e3",
   "metadata": {},
   "source": [
    "### Depth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7dbd43-47dd-4c9c-a160-c6533d8d5181",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_temp_1000 = helper_functions.reconstructed_to_stack(ds, \"thetao\", 300, X_recon_all)\n",
    "helper_functions.plot_average_cluster(recon_temp_50, labels, -2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993c926-c457-4f9c-a206-99d5d358e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_so_1000 = helper_functions.reconstructed_to_stack(ds, \"so\", 1000, X_recon_all)\n",
    "helper_functions.plot_average_cluster(recon_so_1000, labels, -2, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
