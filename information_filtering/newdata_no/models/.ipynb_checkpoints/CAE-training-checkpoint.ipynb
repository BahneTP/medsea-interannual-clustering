{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4951ac0e-4f7b-4c6d-87e8-fa212d1750a9",
   "metadata": {
    "editable": true,
    "id": "4951ac0e-4f7b-4c6d-87e8-fa212d1750a9",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Convolutional Autoencoder\n",
    "\n",
    "This notebook uses a **Convolutional Autoencoder**, which is a neural network designed to compress spatial data like maps into a lower-dimensional **latent space**, and then reconstruct it. \n",
    "\n",
    "In our case, the goal is to learn compact representations of the oceanographic maps while preserving important spatial patterns. This helps reduce noise, ignore missing or irrelevant regions (thanks to the masking), and makes it easier to apply further analysis — for example, clustering similar patterns in the compressed space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b2f90b1-6826-4a2b-896b-f0f59aa9999c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 110274,
     "status": "ok",
     "timestamp": 1750955287591,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "4b2f90b1-6826-4a2b-896b-f0f59aa9999c",
    "outputId": "4dec4adf-f65f-4eea-9962-5fa5887df0ec"
   },
   "outputs": [],
   "source": [
    "!pip install -q -r ../../requirements.txt &> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a9a3b7-de75-47f5-8e47-612a35397ee0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e3c815e-a7d9-46d6-a45d-c64b8ed20a69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1750955461797,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "7e3c815e-a7d9-46d6-a45d-c64b8ed20a69",
    "outputId": "c8c3bc6e-81b3-473b-e58c-d53fa46ceb8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'helper_functions' from '/home/jovyan/spatiotemporal-mining-medsea/information_filtering/newdata/models/../../helper_functions.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "from torchinfo import summary\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import helper_functions\n",
    "import importlib\n",
    "importlib.reload(helper_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c7cb91c-8458-401b-9d69-1858f6be5b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 27\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "trend_removal = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f10a58f-d84f-4be4-abeb-478f20b1b76f",
   "metadata": {},
   "source": [
    "Since our ConvNet works with masked map data, it naturally produces some NaNs — that's expected behavior. To avoid flooding the output with warnings, we've disabled them here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4956a2-917d-4d60-afc2-410b41767582",
   "metadata": {},
   "source": [
    "Again, we include all depths and features for reconstruction. The difference in our ConvNet is that these features are not simply concatenated, but stacked as channels — similar to how RGB channels work in images. This allows the network to capture spatial relationships between features and depths more effectively, which might be important in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11779e19-1403-4924-b480-f5457daaee2e",
   "metadata": {
    "id": "11779e19-1403-4924-b480-f5457daaee2e"
   },
   "source": [
    "## Data Loading & Preprocessing & Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "974f3cdc-1c88-4f71-88ee-bb7d3fe13362",
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1750955397581,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "974f3cdc-1c88-4f71-88ee-bb7d3fe13362"
   },
   "outputs": [],
   "source": [
    "class MaskedDataset(Dataset):\n",
    "    def __init__(self, X, M):\n",
    "        self.X = X\n",
    "        self.M = M\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.M[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6528d6b-b747-4ab5-a844-9922674e0df9",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36cff7de-fbe6-4edb-9c94-5fead6c7a13a",
   "metadata": {
    "executionInfo": {
     "elapsed": 874,
     "status": "ok",
     "timestamp": 1750955398447,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "36cff7de-fbe6-4edb-9c94-5fead6c7a13a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 461\n"
     ]
    }
   ],
   "source": [
    "ds_train = xr.open_dataset(\"../../data/medsea1987to2025_train.nc\")\n",
    "\n",
    "X_np, M_np = helper_functions.preprocessing_conv(ds_train, [\"thetao\", \"so\"], [50, 300, 1000], trend_removal, 1)\n",
    "X_train = torch.tensor(X_np)\n",
    "M_train = torch.tensor(M_np)\n",
    "train_dataset = MaskedDataset(X_train, M_train)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    generator=torch.Generator().manual_seed(SEED)\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ae8353-d314-4f9f-88e6-e38d53c978ef",
   "metadata": {},
   "source": [
    "### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f099302-5bfc-441f-9360-ba3906da7d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set size: 184\n"
     ]
    }
   ],
   "source": [
    "ds_val = xr.open_dataset(\"../../data/medsea1987to2025_val.nc\")\n",
    "\n",
    "X_np, M_np = helper_functions.preprocessing_conv(ds_train, [\"thetao\", \"so\"], [50, 300, 1000], trend_removal, 1)\n",
    "X_val = torch.tensor(X_np)\n",
    "M_val = torch.tensor(M_np)\n",
    "val_dataset = MaskedDataset(X_val, M_val)\n",
    "\n",
    "val_size = int(0.4 * len(val_dataset))\n",
    "_ , val_subset = torch.utils.data.random_split(\n",
    "    val_dataset,\n",
    "    [len(val_dataset) - val_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(SEED)\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_subset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Validation set size: {len(val_subset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed51ed5-9af0-42ad-a1a9-9d9b0b5edfe7",
   "metadata": {
    "id": "4ed51ed5-9af0-42ad-a1a9-9d9b0b5edfe7"
   },
   "source": [
    "## The Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "BmPchZzRmXID",
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1750955534808,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "BmPchZzRmXID"
   },
   "outputs": [],
   "source": [
    "class CAE(nn.Module):\n",
    "    def __init__(self, in_channels, latent_dim=3, dropout_p=0.2, channels=[32, 64, 128], input_shape=(203, 514)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.channels = channels\n",
    "\n",
    "        encoder_layers = []\n",
    "        prev_channels = in_channels\n",
    "        h, w = input_shape\n",
    "\n",
    "        for ch in channels:\n",
    "            encoder_layers += [\n",
    "                nn.Conv2d(prev_channels, ch, kernel_size=3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(ch),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout2d(p=dropout_p),\n",
    "            ]\n",
    "            prev_channels = ch\n",
    "            h = math.floor((h + 2 * 1 - 3) / 2 + 1)  # Conv2d output size formula\n",
    "            w = math.floor((w + 2 * 1 - 3) / 2 + 1)\n",
    "\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        self.unflatten_shape = (channels[-1], h, w)\n",
    "\n",
    "        flat_dim = channels[-1] * h * w\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_enc = nn.Sequential(\n",
    "            nn.Linear(flat_dim, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(1024, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(64, latent_dim)\n",
    "        )\n",
    "        self.fc_dec = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(64, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(1024, flat_dim)\n",
    "        )\n",
    "\n",
    "        decoder_layers = []\n",
    "        rev_channels = list(reversed(channels))\n",
    "        for i in range(len(rev_channels) - 1):\n",
    "            decoder_layers += [\n",
    "                nn.ConvTranspose2d(rev_channels[i], rev_channels[i + 1], kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(rev_channels[i + 1]),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout2d(p=dropout_p),\n",
    "            ]\n",
    "\n",
    "        decoder_layers += [\n",
    "            nn.ConvTranspose2d(rev_channels[-1], in_channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        ]\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        if mask is not None:\n",
    "            x = x * mask\n",
    "        x = self.encoder(x)\n",
    "        z = self.fc_enc(self.flatten(x))\n",
    "        x = self.fc_dec(z)\n",
    "        x = x.view(x.size(0), *self.unflatten_shape)\n",
    "        x = self.decoder(x)\n",
    "        return x[:, :, :self.input_shape[0], :self.input_shape[1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd997601-9a5d-4a69-a879-fe1c9936e87b",
   "metadata": {},
   "source": [
    "### Convolutional Autoencoder\n",
    "\n",
    "We use a simple convolutional autoencoder that compresses the input maps into a small latent vector and reconstructs them back. The encoder reduces spatial resolution through convolutional layers, while the decoder upsamples the data back using transposed convolutions.\n",
    "\n",
    "I experimented with different numbers of layers, kernel sizes, and dropout values. The current setup gave the best trade-off between reconstruction quality and training stability.\n",
    "\n",
    "\n",
    "### Hard Masking\n",
    "- Since many values in the input maps are invalid (e.g. land areas), we apply a **hard mask** before feeding the data into the encoder.\n",
    "- This mask zeroes out irrelevant values, so the model only learns from valid oceanic regions.\n",
    "- During training, the same mask is applied to the loss function to ensure the model is not penalized for errors in masked-out areas.\n",
    "- This is essential for learning robust spatial patterns without being misled by missing or irrelevant data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f29428-7c81-4a80-a415-c051c92d86cd",
   "metadata": {
    "id": "f9f29428-7c81-4a80-a415-c051c92d86cd"
   },
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f36d7cfa-8d22-41f0-a1b0-fb2c31ac8f5e",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1750955398551,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "f36d7cfa-8d22-41f0-a1b0-fb2c31ac8f5e"
   },
   "outputs": [],
   "source": [
    "def masked_mse(x_recon, x_true, mask):\n",
    "    loss = ((x_recon - x_true) ** 2) * mask\n",
    "    return loss.sum() / mask.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696eba88-be08-4790-a145-0a3ddeeffaa2",
   "metadata": {
    "id": "696eba88-be08-4790-a145-0a3ddeeffaa2"
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8b3e058-34ba-439d-aeda-aa0a21e6dd5e",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1750955398556,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "c8b3e058-34ba-439d-aeda-aa0a21e6dd5e"
   },
   "outputs": [],
   "source": [
    "def train(num_epochs: int):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "\n",
    "        for x, mask in train_loader:\n",
    "            x = x.to(device)\n",
    "            mask = mask.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            x_recon = model(x, mask=mask)\n",
    "            loss = masked_mse(x_recon, x, mask)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item() * x.size(0)\n",
    "\n",
    "        train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, mask in val_loader:\n",
    "                x = x.to(device)\n",
    "                mask = mask.to(device)\n",
    "                x_recon = model(x, mask=mask)\n",
    "                loss = masked_mse(x_recon, x, mask)\n",
    "                running_val_loss += loss.item() * x.size(0)\n",
    "\n",
    "        val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd29feb4-1f80-400e-b1f7-a41f47fcace9",
   "metadata": {
    "id": "bd29feb4-1f80-400e-b1f7-a41f47fcace9"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22fecc03-c5a1-49d2-87d0-9b490e8ff0c2",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1750955398545,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "22fecc03-c5a1-49d2-87d0-9b490e8ff0c2"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36mleCQi1wa1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "executionInfo": {
     "elapsed": 224,
     "status": "error",
     "timestamp": 1750955678818,
     "user": {
      "displayName": "Bahne Thiel-Peters",
      "userId": "11084520765208222044"
     },
     "user_tz": -120
    },
    "id": "36mleCQi1wa1",
    "outputId": "618c29cd-1083-48b1-b84a-7a9790043df4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CAE                                      [1, 6, 190, 508]          --\n",
       "├─Sequential: 1-1                        [1, 1024, 3, 8]           --\n",
       "│    └─Conv2d: 2-1                       [1, 32, 95, 254]          1,760\n",
       "│    └─InstanceNorm2d: 2-2               [1, 32, 95, 254]          --\n",
       "│    └─LeakyReLU: 2-3                    [1, 32, 95, 254]          --\n",
       "│    └─Dropout2d: 2-4                    [1, 32, 95, 254]          --\n",
       "│    └─Conv2d: 2-5                       [1, 64, 48, 127]          18,496\n",
       "│    └─InstanceNorm2d: 2-6               [1, 64, 48, 127]          --\n",
       "│    └─LeakyReLU: 2-7                    [1, 64, 48, 127]          --\n",
       "│    └─Dropout2d: 2-8                    [1, 64, 48, 127]          --\n",
       "│    └─Conv2d: 2-9                       [1, 128, 24, 64]          73,856\n",
       "│    └─InstanceNorm2d: 2-10              [1, 128, 24, 64]          --\n",
       "│    └─LeakyReLU: 2-11                   [1, 128, 24, 64]          --\n",
       "│    └─Dropout2d: 2-12                   [1, 128, 24, 64]          --\n",
       "│    └─Conv2d: 2-13                      [1, 256, 12, 32]          295,168\n",
       "│    └─InstanceNorm2d: 2-14              [1, 256, 12, 32]          --\n",
       "│    └─LeakyReLU: 2-15                   [1, 256, 12, 32]          --\n",
       "│    └─Dropout2d: 2-16                   [1, 256, 12, 32]          --\n",
       "│    └─Conv2d: 2-17                      [1, 512, 6, 16]           1,180,160\n",
       "│    └─InstanceNorm2d: 2-18              [1, 512, 6, 16]           --\n",
       "│    └─LeakyReLU: 2-19                   [1, 512, 6, 16]           --\n",
       "│    └─Dropout2d: 2-20                   [1, 512, 6, 16]           --\n",
       "│    └─Conv2d: 2-21                      [1, 1024, 3, 8]           4,719,616\n",
       "│    └─InstanceNorm2d: 2-22              [1, 1024, 3, 8]           --\n",
       "│    └─LeakyReLU: 2-23                   [1, 1024, 3, 8]           --\n",
       "│    └─Dropout2d: 2-24                   [1, 1024, 3, 8]           --\n",
       "├─Flatten: 1-2                           [1, 24576]                --\n",
       "├─Sequential: 1-3                        [1, 3]                    --\n",
       "│    └─Linear: 2-25                      [1, 1024]                 25,166,848\n",
       "│    └─LeakyReLU: 2-26                   [1, 1024]                 --\n",
       "│    └─Dropout: 2-27                     [1, 1024]                 --\n",
       "│    └─Linear: 2-28                      [1, 64]                   65,600\n",
       "│    └─LeakyReLU: 2-29                   [1, 64]                   --\n",
       "│    └─Dropout: 2-30                     [1, 64]                   --\n",
       "│    └─Linear: 2-31                      [1, 3]                    195\n",
       "├─Sequential: 1-4                        [1, 24576]                --\n",
       "│    └─Linear: 2-32                      [1, 64]                   256\n",
       "│    └─LeakyReLU: 2-33                   [1, 64]                   --\n",
       "│    └─Dropout: 2-34                     [1, 64]                   --\n",
       "│    └─Linear: 2-35                      [1, 1024]                 66,560\n",
       "│    └─LeakyReLU: 2-36                   [1, 1024]                 --\n",
       "│    └─Dropout: 2-37                     [1, 1024]                 --\n",
       "│    └─Linear: 2-38                      [1, 24576]                25,190,400\n",
       "├─Sequential: 1-5                        [1, 6, 192, 512]          --\n",
       "│    └─ConvTranspose2d: 2-39             [1, 512, 6, 16]           4,719,104\n",
       "│    └─InstanceNorm2d: 2-40              [1, 512, 6, 16]           --\n",
       "│    └─LeakyReLU: 2-41                   [1, 512, 6, 16]           --\n",
       "│    └─Dropout2d: 2-42                   [1, 512, 6, 16]           --\n",
       "│    └─ConvTranspose2d: 2-43             [1, 256, 12, 32]          1,179,904\n",
       "│    └─InstanceNorm2d: 2-44              [1, 256, 12, 32]          --\n",
       "│    └─LeakyReLU: 2-45                   [1, 256, 12, 32]          --\n",
       "│    └─Dropout2d: 2-46                   [1, 256, 12, 32]          --\n",
       "│    └─ConvTranspose2d: 2-47             [1, 128, 24, 64]          295,040\n",
       "│    └─InstanceNorm2d: 2-48              [1, 128, 24, 64]          --\n",
       "│    └─LeakyReLU: 2-49                   [1, 128, 24, 64]          --\n",
       "│    └─Dropout2d: 2-50                   [1, 128, 24, 64]          --\n",
       "│    └─ConvTranspose2d: 2-51             [1, 64, 48, 128]          73,792\n",
       "│    └─InstanceNorm2d: 2-52              [1, 64, 48, 128]          --\n",
       "│    └─LeakyReLU: 2-53                   [1, 64, 48, 128]          --\n",
       "│    └─Dropout2d: 2-54                   [1, 64, 48, 128]          --\n",
       "│    └─ConvTranspose2d: 2-55             [1, 32, 96, 256]          18,464\n",
       "│    └─InstanceNorm2d: 2-56              [1, 32, 96, 256]          --\n",
       "│    └─LeakyReLU: 2-57                   [1, 32, 96, 256]          --\n",
       "│    └─Dropout2d: 2-58                   [1, 32, 96, 256]          --\n",
       "│    └─ConvTranspose2d: 2-59             [1, 6, 192, 512]          1,734\n",
       "==========================================================================================\n",
       "Total params: 63,066,953\n",
       "Trainable params: 63,066,953\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 3.10\n",
       "==========================================================================================\n",
       "Input size (MB): 2.32\n",
       "Forward/backward pass size (MB): 29.37\n",
       "Params size (MB): 252.27\n",
       "Estimated Total Size (MB): 283.95\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# torch.cuda.reset_peak_memory_stats()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CAE(\n",
    "    in_channels=X_np.shape[1],\n",
    "    latent_dim=3, dropout_p=0.2,\n",
    "    channels= [32, 64, 128, 256, 512, 1024],\n",
    "    input_shape=(190,508)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0004)\n",
    "\n",
    "summary(model, input_size=(1, X_np.shape[1], 190, 508))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PUQ6KPZs2CZL",
   "metadata": {
    "editable": true,
    "id": "PUQ6KPZs2CZL",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/800 | Train Loss: 1.0021 | Val Loss: 0.9592\n",
      "Epoch 20/800 | Train Loss: 0.7028 | Val Loss: 0.6504\n",
      "Epoch 30/800 | Train Loss: 0.6167 | Val Loss: 0.5751\n",
      "Epoch 40/800 | Train Loss: 0.5446 | Val Loss: 0.4879\n",
      "Epoch 50/800 | Train Loss: 0.5178 | Val Loss: 0.4614\n",
      "Epoch 60/800 | Train Loss: 0.4861 | Val Loss: 0.4160\n",
      "Epoch 70/800 | Train Loss: 0.4605 | Val Loss: 0.3938\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 800\n",
    "train_losses, val_losses = train(num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68549efe-8ccf-4412-a9e6-0868f92d4e68",
   "metadata": {
    "id": "68549efe-8ccf-4412-a9e6-0868f92d4e68"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b7c521-adda-4880-a83d-a9a10f780db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_functions.plot_metrics([(train_losses, \"Train Loss\"), (val_losses, \"Validation Loss\")], \"Loss (MSE)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0900f0b-2057-4fc5-b583-a0a64b532819",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"CAE.pth\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7749099,
     "sourceId": 12294777,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7749153,
     "sourceId": 12301699,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 387008,
     "modelInstanceId": 366113,
     "sourceId": 451267,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
