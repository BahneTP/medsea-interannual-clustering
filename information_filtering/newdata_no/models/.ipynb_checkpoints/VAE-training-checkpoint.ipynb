{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "This notebook makes use of the **Autoencoder**, which is used to reduce the dimensionality of our dataset in a non-linear way. Furthermore, we then apply **k-means Clustering** as in our last notebook in our new created **Latent Space** in lower dimension. We do so, to get rid of less important variables and achieve a better Clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r ../../requirements.txt &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'helper_functions' from '/home/jovyan/spatiotemporal-mining-medsea/information_filtering/newdata_no/models/../../helper_functions.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import FloatSlider\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from ipywidgets import interact, IntSlider\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import helper_functions     # Own file.\n",
    "import importlib\n",
    "importlib.reload(helper_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 27\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_removal = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = xr.open_dataset(\"../../data/medsea1987to2025_train.nc\")\n",
    "\n",
    "z_temp = helper_functions.preprocessing(ds_train, [\"thetao\", \"so\"], [50, 300, 1000], \"location\", trend_removal, 1)\n",
    "X_train = z_temp.values.astype(np.float32)\n",
    "input_dimension = X_train.shape[1]\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(torch.from_numpy(X_train)), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = xr.open_dataset(\"../../data/medsea1987to2025_val.nc\")\n",
    "\n",
    "z_temp = helper_functions.preprocessing(ds_train, [\"thetao\", \"so\"], [50, 300, 1000], \"location\", trend_removal, 1)\n",
    "X_val = z_temp.values.astype(np.float32)\n",
    "val_dataset = TensorDataset(torch.from_numpy(X_val))\n",
    "\n",
    "val_size = int(0.4 * len(val_dataset))\n",
    "_ , val_subset = random_split(\n",
    "    val_dataset,\n",
    "    [len(val_dataset) - val_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(SEED)\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder for mean and logvar\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(512, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.fc_mean = nn.Linear(32, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(32, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(32, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Linear(512, input_dim)\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z_mean = self.fc_mean(h)\n",
    "        z_logvar = self.fc_logvar(h)\n",
    "        z = self.reparameterize(z_mean, z_logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z_mean, z_logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs: int, kl_annealing_epochs: int = 50, bint = 100):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_recon_list = []\n",
    "    train_kl_list = []\n",
    "    val_recon_list = []\n",
    "    val_kl_list = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        beta = min(bint, epoch / kl_annealing_epochs * bint)\n",
    "\n",
    "        model.train()\n",
    "        running_train_recon = 0.0\n",
    "        running_train_kl = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            x = batch[0].to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            x_recon, z_mean, z_logvar = model(x)\n",
    "\n",
    "            kl_loss = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp(), dim=1)\n",
    "            kl_loss = torch.mean(kl_loss)\n",
    "\n",
    "            recon_loss = reconstruction_loss_fn(x_recon, x)\n",
    "\n",
    "            loss = recon_loss + beta * kl_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_recon += recon_loss.item() * x.size(0)\n",
    "            running_train_kl += kl_loss.item() * x.size(0)\n",
    "\n",
    "        train_recon = running_train_recon / len(train_loader.dataset)\n",
    "        train_kl = running_train_kl / len(train_loader.dataset)\n",
    "        train_loss = train_recon + beta * train_kl\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_recon_list.append(train_recon)\n",
    "        train_kl_list.append(train_kl)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        running_val_recon = 0.0\n",
    "        running_val_kl = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x = batch[0].to(device).float()\n",
    "                x_recon, z_mean, z_logvar = model(x)\n",
    "\n",
    "                kl_loss = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp(), dim=1)\n",
    "                kl_loss = torch.mean(kl_loss)\n",
    "\n",
    "                recon_loss = reconstruction_loss_fn(x_recon, x)\n",
    "\n",
    "                running_val_recon += recon_loss.item() * x.size(0)\n",
    "                running_val_kl += kl_loss.item() * x.size(0)\n",
    "\n",
    "        val_recon = running_val_recon / len(val_loader.dataset)\n",
    "        val_kl = running_val_kl / len(val_loader.dataset)\n",
    "        val_loss = val_recon + beta * val_kl\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        val_recon_list.append(val_recon)\n",
    "        val_kl_list.append(val_kl)\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "                f\"β: {beta:.3f} | \"\n",
    "                f\"Train Loss: {train_loss:.4f} (Recon: {train_recon:.4f}, KL: {train_kl:.4f}) | \"\n",
    "                f\"Val Loss: {val_loss:.4f} (Recon: {val_recon:.4f}, KL: {val_kl:.4f})\"\n",
    "            )\n",
    "\n",
    "    return train_losses, val_losses, train_recon_list, val_recon_list, train_kl_list, val_kl_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Training using device: {device}')\n",
    "\n",
    "model = VariationalAutoencoder(input_dim=input_dimension, latent_dim=3, dropout=0.2).to(device)\n",
    "model = model.float()\n",
    "\n",
    "summary(model, input_size=(1, X_train.shape[1]))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "reconstruction_loss_fn = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/800 | β: 0.002 | Train Loss: 0.6868 (Recon: 0.6815, KL: 2.3280) | Val Loss: 0.6344 (Recon: 0.6289, KL: 2.4220)\n",
      "Epoch 20/800 | β: 0.005 | Train Loss: 0.6477 (Recon: 0.6343, KL: 2.8201) | Val Loss: 0.6063 (Recon: 0.5937, KL: 2.6680)\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, train_recons, val_recons, train_kls, val_kls = train(800,200, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_functions.plot_metrics([(train_losses, \"Train Loss\"), (val_losses, \"Validation Loss\")], \"Loss\")\n",
    "helper_functions.plot_metrics([(train_recons, \"Train MSE-Scores\"), (val_recons, \"Validation MSE-Scores\")], \"MSE Score\")\n",
    "helper_functions.plot_metrics([(train_kls, \"Train KL-Scores\"), (val_kls, \"Validation KL-Scores\")], \"KL-Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "latents_mu = []\n",
    "latents_logvar = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        x = batch[0].to(device).float()\n",
    "        _, mu, logvar = model(x)\n",
    "        latents_mu.append(mu.cpu())\n",
    "        latents_logvar.append(logvar.cpu())\n",
    "\n",
    "mu_all = torch.cat(latents_mu, dim=0)           # shape: (n_samples, latent_dim)\n",
    "logvar_all = torch.cat(latents_logvar, dim=0)   # shape: (n_samples, latent_dim)\n",
    "\n",
    "# Statistics\n",
    "mu_std = mu_all.std(dim=0)\n",
    "logvar_mean = logvar_all.mean(dim=0)\n",
    "logvar_std = logvar_all.std(dim=0)\n",
    "\n",
    "print(\"Std of mu per latent dim:\")\n",
    "print(mu_std)\n",
    "\n",
    "print(\"\\nMean of logvar per latent dim:\")\n",
    "print(logvar_mean)\n",
    "\n",
    "print(\"\\nStd of logvar per latent dim:\")\n",
    "print(logvar_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"VAE.pth\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7749099,
     "sourceId": 12294777,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7749153,
     "sourceId": 12301699,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
